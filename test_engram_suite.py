import torch
import numpy as np
import sys
import os
from transformers import AutoTokenizer

# ç¡®ä¿èƒ½å¯¼å…¥æœ¬åœ°çš„ engram åŒ…
sys.path.append(os.getcwd())

from engram.config import EngramConfig
from engram.tokenizer import EngramTokenizer
from engram.modules import EngramModule


def test_engram_workflow():
    print("=" * 50)
    print("ğŸš€ å¼€å§‹ Engram æ¨¡å—å•å…ƒæµ‹è¯•")
    print("=" * 50)

    # ------------------------------------------------------
    # 1. å‡†å¤‡ç¯å¢ƒ
    # ------------------------------------------------------
    print("\n[Step 1] åˆå§‹åŒ–é…ç½®ä¸ Tokenizer...")

    # ä¸ºäº†æµ‹è¯•æ–¹ä¾¿ï¼Œæˆ‘ä»¬ç”¨ gpt2 çš„ tokenizer (æ¯”è¾ƒå°)
    try:
        hf_tokenizer = AutoTokenizer.from_pretrained("gpt2")
        hf_tokenizer.pad_token = hf_tokenizer.eos_token
    except:
        print("âŒ éœ€è¦å®‰è£… transformers å’Œä¸‹è½½ gpt2 tokenizer")
        return

    # é…ç½®ï¼šæ¨¡æ‹Ÿä¸€ä¸ª Dense æ¨¡å‹ (hc_mult=1)ï¼Œå¼€å¯ CPU Offload
    config = EngramConfig(
        hidden_size=512,  # æ¨¡æ‹Ÿä¸€ä¸ªå°æ¨¡å‹
        engram_vocab_size=len(hf_tokenizer),
        max_ngram_size=3,
        n_embed_per_ngram=64,  # å°ä¸€ç‚¹æ–¹ä¾¿è§‚å¯Ÿ
        n_head_per_ngram=4,
        hc_mult=1,  # å…³é”®ï¼šDense æ¨¡å¼
        seed=42,
    )

    # åˆå§‹åŒ– Engram Tokenizer (æ¨¡æ‹Ÿæ’å…¥åˆ°ç¬¬ 2 å±‚)
    layer_id = 2
    engram_tokenizer = EngramTokenizer(config, hf_tokenizer, layer_ids=[layer_id])
    print("âœ… Tokenizer åˆå§‹åŒ–æˆåŠŸ")

    # ------------------------------------------------------
    # 2. æµ‹è¯•å“ˆå¸Œè®¡ç®— (CPU é€»è¾‘)
    # ------------------------------------------------------
    print("\n[Step 2] æµ‹è¯•å“ˆå¸Œè®¡ç®— (Hash Calculation)...")
    text = ["Hello world, this is a test for Engram.", "Short sentence."]
    hf_enc = hf_tokenizer(
        text, return_tensors="np", padding=True, truncation=True, max_length=20
    )
    input_ids = hf_enc["input_ids"]

    print(f"   Input shape: {input_ids.shape}")

    # æ ¸å¿ƒæµ‹è¯•ï¼šè®¡ç®— Hash
    hash_ids = engram_tokenizer.compress_and_hash(input_ids, layer_id=layer_id)

    # éªŒè¯ç»´åº¦: [Batch, Seq, Num_Heads_Total]
    # Total Heads = (max_ngram_size - 1) * n_head_per_ngram
    # è¿™é‡Œ max_ngram=3 (å³2-gram, 3-gram), head=4 => total=8
    expected_heads = (config.max_ngram_size - 1) * config.n_head_per_ngram
    print(f"   Hash shape:  {hash_ids.shape}")

    assert hash_ids.shape == (
        input_ids.shape[0],
        input_ids.shape[1],
        expected_heads,
    ), f"âŒ å“ˆå¸Œç»´åº¦é”™è¯¯! é¢„æœŸ (B, L, {expected_heads}), å®é™… {hash_ids.shape}"
    print("âœ… å“ˆå¸Œç»´åº¦æ ¡éªŒé€šè¿‡")

    # ------------------------------------------------------
    # 3. åˆå§‹åŒ–æ¨¡å—ä¸é›¶åˆå§‹åŒ–æ£€æŸ¥
    # ------------------------------------------------------
    print("\n[Step 3] åˆå§‹åŒ– EngramModule ä¸ é›¶åˆå§‹åŒ–æ£€æŸ¥...")
    vocab_sizes = engram_tokenizer.vocab_distributions[layer_id]
    module = EngramModule(config, vocab_sizes)

    # æ£€æŸ¥ Embedding æ˜¯å¦åœ¨ CPU (å› ä¸º cpu_offload=True)
    is_on_cpu = module.memory.embedding.weight.device.type == "cpu"
    print(f"   Embedding is on CPU? {is_on_cpu}")
    assert is_on_cpu, "âŒ é…ç½®äº† Offload ä½† Embedding å´åœ¨ GPU ä¸Š"

    # æ£€æŸ¥å·ç§¯æƒé‡æ˜¯å¦ä¸º 0
    conv_weight_sum = module.conv.weight.abs().sum().item()
    print(f"   Conv weight L1 norm: {conv_weight_sum}")
    assert conv_weight_sum == 0, "âŒ é›¶åˆå§‹åŒ–å¤±è´¥ï¼å·ç§¯æƒé‡ä¸ä¸º 0ï¼Œè¿™ä¼šç ´åé¢„è®­ç»ƒæ¨¡å‹ã€‚"
    print("âœ… é›¶åˆå§‹åŒ–æ ¡éªŒé€šè¿‡")

    # ------------------------------------------------------
    # 4. Forward Pass (GPU æ¨ç†)
    # ------------------------------------------------------
    print("\n[Step 4] å‰å‘ä¼ æ’­æµ‹è¯• (Forward Pass)...")

    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"   Testing on device: {device}")

    # å°†æ¨¡å—é Embedding éƒ¨åˆ†ç§»åˆ° GPU
    module.to(device)

    # æ„é€  Dummy Hidden States [B, L, D]
    B, L = input_ids.shape
    hidden_states = torch.randn(B, L, config.hidden_size).to(device)

    # æ„é€  Hash IDs (éœ€è¦è½¬ä¸º Tensor, ä½†ä¸éœ€è¦æ‰‹åŠ¨ .to(device) å› ä¸º module å†…éƒ¨å¤„ç† offload)
    # ä½†ä¸ºäº†æ¨¡æ‹ŸçœŸå® DataLoaderï¼Œæˆ‘ä»¬é€šå¸¸ä¼ è¿‡æ¥ tensor
    hash_tensor = torch.from_numpy(hash_ids).long().to(device)

    # RUN
    try:
        # offload embedding to CPU
        module.memory.cpu()

        output = module(hidden_states, hash_tensor)
        print(f"   Output shape: {output.shape}")

        # éªŒè¯ç»´åº¦ä¸å˜ (Residual é€‚é…)
        assert (
            output.shape == hidden_states.shape
        ), f"âŒ è¾“å‡ºç»´åº¦é”™è¯¯! é¢„æœŸ {hidden_states.shape}, å®é™… {output.shape}"

        # éªŒè¯åˆå§‹è¾“å‡ºå€¼æå° (å› ä¸ºé›¶åˆå§‹åŒ–)
        # æ³¨æ„ï¼šç”±äº Gating çš„ sigmoid å’Œ value_proj çš„åˆå§‹åŒ–ï¼Œè¾“å‡ºä¸ä¸€å®šæ˜¯çº¯ 0ï¼Œä½†åº”è¯¥éå¸¸å°
        # æˆ–è€…å¦‚æœæ˜¯ Conv åçš„æ®‹å·®ï¼Œåº”è¯¥æ˜¯ 0 (å–å†³äºå…·ä½“å®ç°ï¼ŒDemoä¸­æ˜¯ value + conv(value))
        # æˆ‘ä»¬è¿™é‡Œåªæ£€æŸ¥æ˜¯å¦æ•°å€¼çˆ†ç‚¸
        out_mean = output.abs().mean().item()
        print(f"   Output mean abs value: {out_mean:.6f}")
        if out_mean > 0.1:
            print("âš ï¸ è­¦å‘Š: åˆå§‹è¾“å‡ºå€¼è¾ƒå¤§ï¼Œå¯èƒ½ä¼šå¯¹ä¸»å¹²æ¨¡å‹äº§ç”Ÿå†²å‡»")
        else:
            print("âœ… åˆå§‹è¾“å‡ºå€¼å¤„äºå®‰å…¨èŒƒå›´")

        print("âœ… å‰å‘ä¼ æ’­æˆåŠŸ")

    except Exception as e:
        print(f"âŒ å‰å‘ä¼ æ’­å´©æºƒ: {e}")
        import traceback

        traceback.print_exc()

    # ------------------------------------------------------
    # 5. åå‘ä¼ æ’­æµ‹è¯• (Gradient Flow)
    # ------------------------------------------------------
    print("\n[Step 5] åå‘ä¼ æ’­æµ‹è¯• (Backward Pass)...")
    try:
        loss = output.sum()
        loss.backward()

        # æ£€æŸ¥ Embedding æ˜¯å¦æœ‰æ¢¯åº¦
        # æ³¨æ„ï¼šEmbedding åœ¨ CPU ä¸Šï¼ŒPyTorch æ”¯æŒ CPU->GPU çš„æ¢¯åº¦å›ä¼ å—ï¼Ÿ
        # PyTorch çš„ Embedding å¦‚æœåœ¨ CPUï¼Œinput åœ¨ GPUï¼Œbackward æ—¶é€šå¸¸ä¹Ÿæ˜¯ OK çš„
        embed_grad = module.memory.embedding.weight.grad

        if embed_grad is not None:
            grad_norm = embed_grad.norm().item()
            print(f"   Embedding gradient norm: {grad_norm}")
            print("âœ… æ¢¯åº¦å›ä¼ æˆåŠŸ (CPU Embedding æ¥æ”¶åˆ°äº†æ¢¯åº¦)")
        else:
            print("âŒ Embedding æ²¡æœ‰æ¥æ”¶åˆ°æ¢¯åº¦!")

    except Exception as e:
        print(f"âŒ åå‘ä¼ æ’­å´©æºƒ: {e}")

    print("\n" + "=" * 50)
    print("ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼å¦‚æœå…¨æ˜¯ âœ…ï¼Œè¯´æ˜ Engram æ¨¡å—å¯ç”¨ã€‚")
    print("=" * 50)


if __name__ == "__main__":
    test_engram_workflow()
